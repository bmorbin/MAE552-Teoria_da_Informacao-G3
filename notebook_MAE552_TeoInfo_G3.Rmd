---
lang: pt-BR
title: ""
output: 
  html_document:
    code_folding: show
    number_sections: true
    css: "settings/style.css"
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: false
      smooth_scroll: false
    df_print: paged
    highlight: breezedark
    includes:
      in_header: 
        - settings/before.html
      after_body:
        - settings/code.html
---
<p style="margin:0!important; user-select: none;"></p>
```{r include=FALSE}
source('settings/setup.R')
source('settings/plots_style.R')
```

```{r echo=F}
par(bg="#14141400",
    fg="white",
    col="white",
    col.axis="white",
    col.lab="white",
    col.main="white",
    col.sub="white"
)
```


<h3>Descrição do estudo</h3>

Now, the connection between clusterization and entropy arises from the idea that when you successfully cluster a dataset, you are effectively reducing the uncertainty or randomness within each cluster. A well-defined cluster contains data points that are similar to each other and dissimilar to points in other clusters. This reduction in uncertainty can be seen as a form of data compression.

When you have a dataset with well-separated clusters, you can encode each cluster with a shorter representation (e.g., a cluster centroid or a label) instead of encoding each individual data point. This compression reduces the amount of information required to represent the dataset as a whole. Consequently, the entropy of the clustered dataset is lower than the entropy of the original, unclustered dataset.

In other words, clusterization can be seen as a form of data compression that aims to maximize the compression ratio by grouping similar data points together, thus reducing the entropy of the data.

# Problema

The entropy formula itself is not directly used to separate data into clusters. Instead, it is used to quantify the uncertainty or randomness within a cluster or a distribution. To separate data into clusters, you would typically use clustering algorithms or techniques such as K-means, hierarchical clustering, or DBSCAN, among others.

However, once you have obtained the clusters using a clustering algorithm, you can calculate the entropy of each cluster to assess the degree of uncertainty or randomness within that cluster. Here's a general approach:

1. Perform Clustering: Apply a clustering algorithm of your choice to partition the data into clusters. Each data point will be assigned to a specific cluster based on some similarity or distance metric.

2. Calculate Cluster Entropy: Once you have the clusters, you can calculate the entropy for each cluster individually. To do this, follow these steps:
   a. For each cluster, compute the probability distribution of the classes or values within that cluster. This distribution represents the relative frequencies of different classes or values within the cluster.
   b. Use the entropy formula (H(X) = - Σ P(x) * log2(P(x))) to calculate the entropy of each cluster based on its probability distribution.
   c. The entropy value will indicate the level of uncertainty or randomness within each cluster. Lower entropy values indicate more homogeneous clusters, where data points are similar to each other and have a predictable distribution, while higher entropy values indicate more diverse or mixed clusters.

3. Analyze Results: Examine the entropy values of the clusters to gain insights into the clustering quality. Lower entropy clusters are generally considered more well-defined, while higher entropy clusters may indicate more ambiguity or overlap between data points.

Keep in mind that clustering and entropy calculation are separate steps in the analysis. Clustering determines the assignment of data points to clusters, while entropy provides a measure of uncertainty or randomness within each cluster.

# Proposta 

Information gain is another concept from information theory that can be used to evaluate the quality of clustering or the effectiveness of features in separating data into clusters. It measures the reduction in entropy achieved by partitioning the data based on a particular feature or attribute.

Here's how information gain can be used in the context of clustering:

1. Calculate the Initial Entropy:
   Calculate the entropy of the target variable or the distribution of classes in the entire dataset before clustering. This will serve as the baseline entropy.

2. Perform Clustering:
   Apply a clustering algorithm to partition the data into clusters based on a set of features or attributes. Each data point will be assigned to a specific cluster.

3. Calculate Cluster Entropy:
   For each cluster obtained from the clustering algorithm, calculate the entropy of the target variable within that cluster. This will involve computing the probability distribution of classes within each cluster and then calculating the entropy using the formula: H(X) = - Σ P(x) * log2(P(x)).

4. Calculate Information Gain:
   Information gain is calculated by comparing the initial entropy (step 1) with the entropy of each cluster (step 3). The information gain achieved by partitioning the data based on a particular feature or attribute is given by:
   
   Information Gain = Initial Entropy - Σ (Proportion of data in each cluster * Cluster Entropy)

   The proportion of data in each cluster can be determined by dividing the number of data points in the cluster by the total number of data points.

5. Evaluate Information Gain:
   Higher information gain indicates that the feature or attribute used for clustering has effectively reduced the uncertainty or randomness within the clusters. It suggests that the chosen feature provides valuable information for separating the data into distinct clusters.

6. Iterative Feature Selection:
   You can repeat steps 2 to 5 with different features or attributes to compare their information gain values. This can help in identifying the most informative features for clustering or in prioritizing the order of feature selection.

Information gain is particularly useful in decision tree-based clustering algorithms, where features are recursively selected to optimize the separation of data into clusters. It helps in identifying the most discriminative features that contribute the most to cluster formation.

## Roteiro

## Código

```{r}
# Carregando pacotes
library(tidyverse)
library(cluster)
```

### Conjunto de dados

```{r}
load("glioma.RData") # geneInfo ; gliomaGSE52009 ; targetInfoGlioma
glioma <- gliomaGSE52009[1:500,]; as.data.frame(glioma)
info <- targetInfoGlioma; rownames(info) <- NULL; info |> select(colnames(info[,-1]),FileName)
```

```{r}
str(info[,-1]) # ignorando a coluna FileName
info$gender <- factor(info$gender); levels(info$gender)
```

```{r, out.width="60%", class.source = 'fold-hide'}
ggplot(info, aes(x=reorder(gender, -table(gender)[gender])))+
  geom_bar(aes(fill=gender), color="transparent")+
  scale_fill_manual(values=c(male="#3E67A3",female="#A34336",unknown="#7F8F85"))+
  geom_text(stat = 'count', aes(label = paste0(round((after_stat(count)/sum(after_stat(count)))*100), "%")), vjust = -0.5) +
  scale_y_continuous(expand = expansion(mult=c(0,.2)))+
  labs(x=NULL,y=NULL, title="Gênero")+
  guides(fill="none")
```

```{r}
info$diagnostic <- factor(info$diagnostic); levels(info$diagnostic)
```

```{r, out.width="80%", class.source = 'fold-hide'}
ggplot(info, aes(x=reorder(diagnostic, -table(diagnostic)[diagnostic])))+
  geom_bar(aes(fill=diagnostic), color="transparent")+
  scale_fill_grey(end = 0.9, start=.5)+
  geom_text(stat = 'count', aes(label = paste0(round((after_stat(count)/sum(after_stat(count)))*100), "%")), vjust = -0.5) +
  scale_y_continuous(expand = expansion(mult=c(0,.15)))+
  labs(x=NULL,y=NULL, title="Diagnóstico")+
  theme(axis.text = element_text(angle = 45,hjust = 1, vjust=0))+
  guides(fill="none")
```

```{r}
range(info$age)
cat(paste0(sum(info$age<=0)," entradas inválidas para idade  => ",round(sum(info$age<=0)/nrow(info)*100,2), "% da amostra")) # Porcentagem de dados inválidos para idade
range(info[which(info$age>0),]$age)
```

```{r, class.source = 'fold-hide'}
ggplot(subset(info[which(info$age>0),]), aes(x = age, y = after_stat(density))) +
  geom_histogram(aes(y = ..density..), fill = "skyblue", color = "#0c0c0c", binwidth = 5, alpha = 0.9) +
  geom_density(color = "cyan", linetype = "solid", linewidth = 1, fill="transparent") +
  labs(x = "Idade", y = "Frequência relativa", subtitle ="(somente dados válidos)", title="Distribuição de idade na amostra")+
  scale_y_continuous(expand = expansion(mult=c(0,.20)))
```

```{r}
unique(info$datasetId)
unique(info$tissue)
```

### Agrupamento dos genes

```{r}
mg <- glioma
nmg <- scale(t(mg))
nmg <- t(nmg)
d<-dist(nmg,method = "euclidean")
```

```{r}
hc1<-hclust(d,method="ward.D")
plot(hc1)
```


### Distribuição unificada por cluster

### Discriminação de características

# Conclusão

